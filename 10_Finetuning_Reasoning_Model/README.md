<p align = "center" draggable=â€falseâ€ ><img src="https://github.com/AI-Maker-Space/LLM-Dev-101/assets/37101144/d1343317-fa2f-41e1-8af1-1dbb18399719" 
     width="200px"
     height="auto"/>
</p>

## <h1 align="center" id="heading">Session 10: Fine-tuning a Reasoning Model</h1>

| ğŸ¤“ Pre-work | ğŸ“° Session Sheet | âºï¸ Recording     | ğŸ–¼ï¸ Slides        | ğŸ‘¨â€ğŸ’» Repo         | ğŸ“ Homework      | ğŸ“ Feedback       |
|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|
| [Session 10: Pre-Work](https://www.notion.so/Session-10-Fine-Tuning-LLMs-Reasoning-Models-1c8cd547af3d815dba2cf9d2b4c0a325?pvs=4#1c8cd547af3d81dc9d7bc1fb2ba9f9cd) | [Session 10: Fine-Tuning LLMs & Reasoning Models ](https://www.notion.so/Session-10-Fine-Tuning-LLMs-Reasoning-Models-1c8cd547af3d815dba2cf9d2b4c0a325) | [Recording](https://us02web.zoom.us/rec/share/tTywDHtoclKDsQVpslumuMISE9Q2kdht_ORTy-fjAgKXRDSmcmZ2ceb5-B6IiNjN.4EL-SfqznEhcTdSs) (.Xn6c0$S)  | [Session 10 Slides](https://www.canva.com/design/DAGjaVT_Bh0/QTzaTZGFTkQiHLylBTjpWw/edit?utm_content=DAGjaVT_Bh0&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton) | You are here! | [Session 10: Fine-Tuning LLMs & Reasoning Models](https://forms.gle/3NHvkcoDm6LyWhWQ9) | [AIE6 Feedback 5/1](https://forms.gle/VrY27ndkZdNEF5nFA) |


In today's assignment, we'll be fine-tuning Llama 3.1 8B to create a reasoning model!

- ğŸ¤ Breakout Room #1
  - Overview of PEFT and LoRA
  - Installation
  - Unsloth Setup
  - Task 1: Loading the Model
    - Overview of Quantization
    - Block-wise k-bit Quantization
    - Initializing LoRA Config

- ğŸ¤ Breakout Room #2
  - Task 1: Loading Data
  - Task 2: Training Setup
  - Task 3: GRPO Training Loop
  - Task 4: Model Evaluation
    
The notebook Colab link is located [here](https://colab.research.google.com/drive/18jF-pOlz-cFt0SkHVQ_9PBuGAH0VumrG?usp=sharing)

## Ship ğŸš¢

The completed notebook!

#### ğŸ—ï¸ BONUS ACTIVITY (FULL MARKS IF COMPLETED IN LIEU OF ABOVE NOTEBOOK):

Using the [Open R1 Math Raw](https://huggingface.co/datasets/open-r1/OpenR1-Math-Raw) dataset, fine-tune Llama 3.1 8B with Unsloth to produce a reasoning model using GRPO.

> NOTE: You will need to come up with your own reward functions that correctly reward the model as it learns to reason.

### Deliverables

- A short Loom of the notebook, and a 1min. walkthrough of the application in full

## Share ğŸš€

Make a social media post about your final application!

### Deliverables

- Make a post on any social media platform about what you built!

Here's a template to get you started:

```
ğŸš€ Exciting News! ğŸš€

I am thrilled to announce that I have just built and shipped fine-tuning a reasoning model with GRPO! ğŸ‰ğŸ¤–

ğŸ” Three Key Takeaways:
1ï¸âƒ£ 
2ï¸âƒ£ 
3ï¸âƒ£ 

Let's continue pushing the boundaries of what's possible in the world of AI and question-answering. Here's to many more innovations! ğŸš€
Shout out to @AIMakerspace !

#LangChain #QuestionAnswering #RetrievalAugmented #Innovation #AI #TechMilestone

Feel free to reach out if you're curious or would like to collaborate on similar projects! ğŸ¤ğŸ”¥
```
